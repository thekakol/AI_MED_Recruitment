{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dc507733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_predict, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b34bee",
   "metadata": {},
   "source": [
    "### Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704cdb5",
   "metadata": {},
   "source": [
    "The Titanic – Machine Learning from Disaster dataset from Kaggle is one of the most popular beginner-friendly challenges in data science. The goal is to build a model that can predict whether a passenger survived the Titanic disaster based on features such as age, gender, ticket class, number of family members on board, and other related information.\n",
    "\n",
    "Each row in the dataset represents one passenger, and the target column — \"Survived\" — indicates the outcome:\n",
    "\n",
    "1 → the passenger survived\n",
    "\n",
    "0 → the passenger did not survive\n",
    "\n",
    "By analyzing these features and training machine learning models, we aim to uncover patterns that influenced survival and evaluate how accurately our model can predict the outcome for unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af1ccd",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In machine learning projects, understanding your dataset is a crucial step toward building an effective model. The file *\"train_data.csv\"* contains the prepared and optimized data used for training. All essential steps of feature engineering and data preprocessing have already been completed, so you can focus entirely on the core aspects — learning how machine learning works, implementing various models, and optimizing their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af620f",
   "metadata": {},
   "source": [
    "### Data Loading and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49c739",
   "metadata": {},
   "source": [
    "The first step is to load the data from the provided .csv files and extract the numerical features. We use the StandardScaler() to normalize the data within the range of -1 to 1, as many machine learning algorithms perform better on standardized inputs. The training dataset contains the \"Survived\" column — this allows the model to learn patterns and optimize its predictions. The test dataset, on the other hand, does not include the \"Survived\" values; the model will use what it has learned to predict them as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0fa6eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from the CSV file\n",
    "# The file \"train_data.csv\" contains the complete dataset that we’ll split\n",
    "# into training and testing parts (80% for training, 20% for testing).\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Select the columns (features) that the model will use to learn\n",
    "# These include numeric values describing each passenger,\n",
    "# such as ticket class, age, number of relatives, ticket fare, and cabin information.\n",
    "# Additional columns come from previous preprocessing\n",
    "# (for example, encoded titles and embarkation ports).\n",
    "X = data[[\n",
    "    \"Pclass\", \"Age\", \"SibSp\", \"Parch\",\n",
    "    \"Fare\", \"Cabin_quantity\", \"Binary\",\n",
    "    \"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Rev\", \"Dr\", \"Col\", \"Major\", \"Mlle\", \"Ms\",\n",
    "    \"Mme\", \"Don\", \"Sir\", \"Lady\", \"Capt\", \"the Countess\", \"Jonkheer\", \"Dona\",\n",
    "    \"Cherbourg\", \"Queenstown\", \"Southampton\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\"\n",
    "]]\n",
    "\n",
    "# Select the target column (\"Survived\"), which the model will try to predict\n",
    "# 1 means the passenger survived, 0 means they did not\n",
    "y = data[\"Survived\"]\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) parts\n",
    "# This allows us to train the model on one portion and test it on unseen data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StandardScaler object to normalize numeric data\n",
    "# Scaling ensures that features are on a similar scale (for example, between -1 and 1),\n",
    "# which helps many algorithms train faster and perform better.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and apply it to both training and test sets\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafde1bc",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be27e97",
   "metadata": {},
   "source": [
    "With the data properly loaded and scaled, we can now move on to training our first model — the K-Nearest Neighbors (KNN) classifier.\n",
    "If you’re not familiar with the specific models used in this guide, you can find short descriptions and documentation references in the accompanying *\"ML.md\"* file.\n",
    "\n",
    "Each machine learning algorithm has its own hyperparameters — settings that must be chosen manually before training. The choice of both the model type and its hyperparameters is one of the key factors affecting prediction accuracy.\n",
    "\n",
    "While there are advanced libraries such as Optuna that can automatically optimize these parameters, in this tutorial we will focus on the built-in tools provided by scikit-learn.\n",
    "To find the best settings, we’ll use GridSearchCV, and to evaluate model accuracy we’ll rely on standard scikit-learn performance metrics and validation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f9183",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7ea165f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of training data cross-validation (each fold):\n",
      "0.8\n",
      "0.82\n",
      "0.82\n",
      "0.83\n",
      "0.8\n",
      "\n",
      "Cross-validation mean score: 0.814\n",
      "Standard deviation of CV score: 0.012\n"
     ]
    }
   ],
   "source": [
    "# Define a Pipeline that bundles preprocessing (scaling) and the model\n",
    "# The scaler is fit ONLY on the training portion inside each CV fold,\n",
    "# then applied to the validation fold — this prevents data leakage.\n",
    "# We choose \"random\" hiperparameters at our own discretion\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),     # Standardize features inside each CV fold\n",
    "    (\"model\", KNeighborsClassifier(   # K-Nearest Neighbors classifier\n",
    "        n_neighbors = 3,                # Number of neighbors used for prediction\n",
    "        weights='distance',           # Weight controls how much influence each neighbor has when making a prediction.\n",
    "        metric='manhattan'            # Look up different distance formulas in documentation\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the Pipeline on the training data\n",
    "# Internally, the scaler will be fit to X_train and the model will learn from the scaled data.\n",
    "pipe_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using cross-validation WITHOUT leakage\n",
    "# For each fold, the scaler is fit on the fold’s training split and applied to its validation split.\n",
    "cv_score = np.round(cross_val_score(pipe_knn, X_train, y_train), 2)\n",
    "\n",
    "# Display detailed results\n",
    "# Shows accuracy for each fold, the mean accuracy, and the standard deviation.\n",
    "# Lower standard deviation indicates more consistent performance across folds.\n",
    "print(\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3}\")\n",
    "print(f\"Standard deviation of CV score: {np.std(cv_score):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea70b4",
   "metadata": {},
   "source": [
    "A score of around 80% accuracy is already quite good, but the hyperparameters used so far were chosen somewhat at random, without any systematic optimization. Let’s improve that now.\n",
    "\n",
    "To achieve more reliable results, we’ll use GridSearchCV for hyperparameter tuning, combined with RepeatedStratifiedKFold.\n",
    "Without this method, running the same model configuration multiple times can produce slightly different accuracy scores due to the randomness in how the data is split.\n",
    "By using Repeated Stratified K-Fold, we repeat the cross-validation process several times and average the results, leading to a more stable and consistent accuracy estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "60f5ef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 500 folds for each of 48 candidates, totalling 24000 fits\n",
      "Best parameters: {'model__metric': 'manhattan', 'model__n_neighbors': 7, 'model__weights': 'uniform'}\n",
      "Best accuracy (averaged CV): 0.8367\n"
     ]
    }
   ],
   "source": [
    "# Define a grid of hyperparameters to test\n",
    "# Each combination of these parameters will be evaluated to find the best-performing model.\n",
    "param_grid = {\n",
    "    \"model__n_neighbors\": [3, 5, 7, 9, 11, 15],  # Number of neighbors to consider\n",
    "    \"model__weights\": [\"uniform\", \"distance\"],   # How neighbors contribute to the prediction\n",
    "    \"model__metric\": [\"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\"],  # Distance metrics to test\n",
    "}\n",
    "\n",
    "# Set up the cross-validation strategy\n",
    "# RepeatedStratifiedKFold splits the data into several folds while preserving class balance.\n",
    "# The process is repeated multiple times (100 here) to get more stable results.\n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=5,      # Number of folds per repetition\n",
    "    n_repeats=100,   # Number of times to repeat the process\n",
    "    random_state=None  # Random seed (None = random each run)\n",
    ")\n",
    "\n",
    "# Define a Pipeline combining the scaler and the model\n",
    "# This ensures that scaling happens *within* each CV fold, preventing data leakage.\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),           # Standardize data inside each CV fold\n",
    "    (\"model\", KNeighborsClassifier())       # The KNN model to be optimized\n",
    "])\n",
    "\n",
    "# Initialize the Grid Search for the KNN model\n",
    "# GridSearchCV will train and evaluate a model for every combination of parameters in param_grid,\n",
    "# using the defined cross-validation strategy.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe_knn,          # The pipeline (scaler + model)\n",
    "    param_grid=param_grid,       # The grid of parameters to test\n",
    "    scoring=\"accuracy\",          # Metric used to evaluate model performance\n",
    "    cv=rskf,                     # Cross-validation strategy\n",
    "    verbose=1,                   # Display progress in the console\n",
    "    n_jobs=-1                    # Use all available CPU cores for faster processing\n",
    ")\n",
    "\n",
    "# Train (fit) the grid search on the raw training data\n",
    "# The scaler will be fit automatically within each fold.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best results\n",
    "# best_params_ shows which combination performed the best,\n",
    "# best_score_ shows the corresponding average accuracy.\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy (averaged CV): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac0869",
   "metadata": {},
   "source": [
    "As we can see, the initial model used completely unoptimized hyperparameters.\n",
    "In the KNN example, we’ll go through each step of the optimization process using GridSearchCV to demonstrate how it works in detail.\n",
    "For the upcoming models, however, we’ll simply provide the already optimized parameters to keep the focus on comparing their performance rather than repeating the full tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 50 folds for each of 144 candidates, totalling 7200 fits\n",
      "Best parameters: {'model__metric': 'manhattan', 'model__n_neighbors': 7, 'model__p': 1, 'model__weights': 'uniform'}\n",
      "Best accuracy (averaged CV): 0.8381\n"
     ]
    }
   ],
   "source": [
    "# Refine the hyperparameter grid based on previous search results\n",
    "# We now focus on narrower ranges around the most promising values to fine-tune the model.\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_neighbors\": [6, 7, 8],  \n",
    "    \"model__weights\": [\"uniform\"],  \n",
    "    \"model__metric\": [\"manhattan\"], \n",
    "}\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=5,      \n",
    "    n_repeats=100,   \n",
    "    random_state=None  \n",
    ")\n",
    "\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),           \n",
    "    (\"model\", KNeighborsClassifier())     \n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe_knn,         \n",
    "    param_grid=param_grid,     \n",
    "    scoring=\"accuracy\",         \n",
    "    cv=rskf,                 \n",
    "    verbose=1,                 \n",
    "    n_jobs=-1                    \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy (averaged CV): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c9286a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of training data cross-validation (each fold):\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.86\n",
      "0.8\n",
      "\n",
      "Cross-validation mean score: 0.842\n",
      "Standard deviation of CV score: 0.022\n"
     ]
    }
   ],
   "source": [
    "# We now enter optimized parameters into out model and check for improved accuracy score\n",
    "\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),    \n",
    "    (\"model\", KNeighborsClassifier(  \n",
    "        n_neighbors = 7,              \n",
    "        weights='uniform',         \n",
    "        metric='manhattan'        \n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_knn.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(pipe_knn, X_train, y_train), 2)\n",
    "\n",
    "print(\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3}\")\n",
    "print(f\"Standard deviation of CV score: {np.std(cv_score):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38730b95",
   "metadata": {},
   "source": [
    "We achieved an improvement of about 2% in accuracy, meaning our KNN model is now fully optimized.\n",
    "With this baseline established, we can move on to testing other models to see which one delivers the best overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb680c35",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e7c88cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of training data cross-validation (each fold):\n",
      "0.79\n",
      "0.83\n",
      "0.84\n",
      "0.85\n",
      "0.72\n",
      "\n",
      "Cross-validation mean score: 0.806\n",
      "Standard deviation of CV score: 0.048\n"
     ]
    }
   ],
   "source": [
    "# We now move on to the Decision Tree.\n",
    "# Since Decision Trees are not sensitive to feature scaling, we’ll not include data normalization and not use pipeline \n",
    "\n",
    "clf_tree = DecisionTreeClassifier(\n",
    "    max_depth=7,            # Limits how deep the tree can grow (to prevent overfitting)\n",
    "    criterion='log_loss',    # Measures the quality of a split using information gain\n",
    "    min_samples_split=7,    # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=5,     # Minimum number of samples required to be at a leaf node\n",
    "    class_weight=None       # No weighting — all classes are treated equally\n",
    ")\n",
    "\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_tree, X_train, y_train), 2)\n",
    "\n",
    "print(f\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3}\")\n",
    "print(f\"Standard deviation of CV score: {np.std(cv_score):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8518739",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of training data cross-validation (each fold):\n",
      "0.81\n",
      "0.85\n",
      "0.85\n",
      "0.85\n",
      "0.8\n",
      "\n",
      "Cross-validation mean score: 0.832\n",
      "Standard deviation of CV score: 0.022\n"
     ]
    }
   ],
   "source": [
    "# We now move on to the Support Vector Classifier (SVC).\n",
    "# Since SVMs are sensitive to feature scaling, we’ll include data normalization \n",
    "# directly in a Pipeline to ensure proper preprocessing and prevent any data leakage during cross-validation.\n",
    "\n",
    "pipe_svc = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),   \n",
    "    (\"model\", SVC(                  # Support Vector Classifier\n",
    "        kernel=\"rbf\",               # RBF kernel captures non-linear decision boundaries\n",
    "        C=3,                        # Regularization strength (higher = tighter fit to training data)\n",
    "        gamma=\"scale\",              # Kernel width; 'scale' adapts to data variance\n",
    "        class_weight=None           # Treat classes equally (no re-weighting)\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(pipe_svc, X_train, y_train), 2)\n",
    "\n",
    "print(\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {cv_score.mean():.3f}\")\n",
    "print(f\"Standard deviation of CV score: {cv_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b0cfa",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "36ac45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of training data cross-validation (each fold):\n",
      "0.81\n",
      "0.83\n",
      "0.86\n",
      "0.85\n",
      "0.78\n",
      "\n",
      "Cross-validation mean score: 0.826\n",
      "Standard deviation of CV score: 0.029\n"
     ]
    }
   ],
   "source": [
    "# Same with logistic regression \n",
    "\n",
    "pipe_log = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),           # Normalize numeric features per fold\n",
    "    (\"model\", LogisticRegression(           # Logistic Regression for binary classification\n",
    "        C=1,                              # Inverse regularization strength (higher = weaker regularization)\n",
    "        penalty=\"l1\",                       # L1 regularization (drives some coefficients to exactly zero)\n",
    "        solver=\"liblinear\",                 # Solver compatible with L1 penalty\n",
    "        max_iter=1000,                      # Ensure convergence\n",
    "        class_weight=None                   # Treat classes equally\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_log.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(pipe_log, X_train, y_train), 2)\n",
    "\n",
    "print(\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {cv_score.mean():.3f}\")\n",
    "print(f\"Standard deviation of CV score: {cv_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcbb16",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e20e1419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of training data cross-validation (each fold):\n",
      "0.84\n",
      "0.85\n",
      "0.85\n",
      "0.85\n",
      "0.83\n",
      "\n",
      "Cross-validation mean score: 0.844\n",
      "Standard deviation of CV score: 0.008\n"
     ]
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(\n",
    "    max_depth=6,                     # Limit the depth of each tree to prevent overfitting\n",
    "    min_samples_split=6,            # Minimum samples required to split a node\n",
    "    n_estimators=125,                # Number of trees in the forest\n",
    "    min_samples_leaf = 2,\n",
    "    max_features = 'sqrt'              \n",
    ")\n",
    "#{'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 9, 'n_estimators': 175}\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_rf, X_train, y_train), 2)\n",
    "\n",
    "print(f\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3f}\")\n",
    "print(f\"Standard deviation of CV score: {np.std(cv_score):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f19b4",
   "metadata": {},
   "source": [
    "### Models score\n",
    "\n",
    "| Classifier           | Accuracy |\n",
    "|----------------------|----------|\n",
    "| KNN                  | 84.2%    |\n",
    "| Decision Tree        | 80.6%    |\n",
    "| SVM                  | 83.2%    |\n",
    "| Logistic Regression  | 82.6%    |\n",
    "| Random Forest        | 84.4%    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c269221",
   "metadata": {},
   "source": [
    "We achieved the highest accuracy with the Decision Tree model, so now we’ll apply it to the test dataset to see how well it performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1394a1",
   "metadata": {},
   "source": [
    "### Running the Model on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a1e75c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:\n",
      "- Accuracy of KNN Classifier model on test dataset:              0.7584\n",
      "- Accuracy of Decision Tree model on test dataset:    0.8258\n",
      "- Accuracy of SVC model on test dataset:              0.8146\n",
      "- Accuracy of Logistic Regression model on test dataset:    0.8202\n",
      "- Accuracy of Random Forest model on test dataset:    0.8090\n"
     ]
    }
   ],
   "source": [
    "# Predictions: use raw X_test\n",
    "# (Pipelines include preprocessing, so no manual scaling of X_test)\n",
    "y_pred_knn  = pipe_knn.predict(X_test)   # nie X_scaled_test\n",
    "y_pred_svc  = pipe_svc.predict(X_test)   # nie X_scaled_test\n",
    "y_pred_log  = pipe_log.predict(X_test)   # nie X_scaled_test\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "y_pred_rf   = clf_rf.predict(X_test)\n",
    "\n",
    "# Model evaluation: calculate accuracy for each model separately\n",
    "acc_knn  = accuracy_score(y_test, y_pred_knn)\n",
    "acc_tree = accuracy_score(y_test, y_pred_tree)\n",
    "acc_svc  = accuracy_score(y_test, y_pred_svc)\n",
    "acc_log  = accuracy_score(y_test, y_pred_log)\n",
    "acc_rf   = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy on test set:\")\n",
    "print(f\"- Accuracy of KNN Classifier model on test dataset:              {acc_knn:.4f}\")\n",
    "print(f\"- Accuracy of Decision Tree model on test dataset:    {acc_tree:.4f}\")\n",
    "print(f\"- Accuracy of SVC model on test dataset:              {acc_svc:.4f}\")\n",
    "print(f\"- Accuracy of Logistic Regression model on test dataset:    {acc_log:.4f}\")\n",
    "print(f\"- Accuracy of Random Forest model on test dataset:    {acc_rf:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
